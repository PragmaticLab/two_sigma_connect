{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load deps\n",
    "\n",
    "# general-purpose imports\n",
    "import feather # may need to put this in your .bashrc: export MACOSX_DEPLOYMENT_TARGET=10.10\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML deps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Image processing deps\n",
    "from PIL import Image\n",
    "from StringIO import StringIO\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Control those annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating features from the listing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# References: http://www.racketracer.com/2016/07/06/pandas-in-parallel/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import uuid\n",
    "from multiprocessing import Pool\n",
    "\n",
    "num_partitions = 250 #number of partitions to split dataframe\n",
    "num_cores = 7 #number of cores on your machine\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def append_image_features(data):\n",
    "    img_features = data['photos'].map(lambda photo_album: get_image_features(photo_album))\n",
    "    img_df = pd.DataFrame({\n",
    "            'mean_red': np.array([feature_dict['mean_red'] for feature_dict in img_features]),\n",
    "            'mean_green': np.array([feature_dict['mean_green'] for feature_dict in img_features]),\n",
    "            'mean_blue': np.array([feature_dict['mean_blue'] for feature_dict in img_features]),\n",
    "            'std_red': np.array([feature_dict['std_red'] for feature_dict in img_features]),\n",
    "            'std_green': np.array([feature_dict['std_green'] for feature_dict in img_features]),\n",
    "            'std_blue': np.array([feature_dict['std_blue'] for feature_dict in img_features]),\n",
    "            'img_resolution': np.array([feature_dict['img_resolution'] for feature_dict in img_features])\n",
    "        })\n",
    "    return img_df\n",
    "\n",
    "def get_image_features(photo_url_list):\n",
    "    \"\"\"\n",
    "    Create one row of features for a collection of\n",
    "    images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write a temp file to disk to track progress\n",
    "    fname = '/Users/jlamb/repos/sandbox/tmp/' + str(uuid.uuid1())\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('x')\n",
    "    \n",
    "    if len(photo_url_list) > 0:\n",
    "        \n",
    "        try: \n",
    "            # Set up collectors\n",
    "            mean_red = []\n",
    "            mean_green = []\n",
    "            mean_blue = []\n",
    "            std_red = []\n",
    "            std_green = []\n",
    "            std_blue = []\n",
    "            img_resolution = []\n",
    "\n",
    "            # TESTING: Just use first image for now\n",
    "            photo_url_list = [photo_url_list[0]]\n",
    "            for url in photo_url_list:\n",
    "\n",
    "                # Get photo (http://stackoverflow.com/questions/7391945/how-do-i-read-image-data-from-a-url-in-python)\n",
    "                url = url\n",
    "                response = requests.get(url)\n",
    "                img = np.array(Image.open(StringIO(response.content)))\n",
    "\n",
    "                # Mean value by channel\n",
    "                mean_red.append(img[:,0].mean())\n",
    "                mean_green.append(img[:,1].mean())\n",
    "                mean_blue.append(img[:,2].mean())\n",
    "\n",
    "                # standard deviation by channel\n",
    "                std_red.append(img[:,0].std())\n",
    "                std_green.append(img[:,1].std())\n",
    "                std_blue.append(img[:,2].std())\n",
    "\n",
    "                # resolution (num pixels)\n",
    "                img_resolution.append(img.size)\n",
    "\n",
    "            # Summarize \n",
    "            out_dict = {\n",
    "                'mean_red': np.mean(np.array(mean_red)),\n",
    "                'mean_green': np.mean(np.array(mean_green)),\n",
    "                'mean_blue': np.mean(np.array(mean_blue)),\n",
    "                'std_red': np.mean(np.array(std_red)),\n",
    "                'std_green': np.mean(np.array(std_green)),\n",
    "                'std_blue': np.mean(np.array(std_blue)),\n",
    "                'img_resolution': np.mean(np.array(img_resolution))\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            out_dict = {\n",
    "                'mean_red': float('nan'),\n",
    "                'mean_green': float('nan'),\n",
    "                'mean_blue': float('nan'),\n",
    "                'std_red': float('nan'),\n",
    "                'std_green': float('nan'),\n",
    "                'std_blue': float('nan'),\n",
    "                'img_resolution': float('nan')\n",
    "            }\n",
    "            \n",
    "        \n",
    "    else:\n",
    "    \n",
    "        out_dict = {\n",
    "            'mean_red': float('nan'),\n",
    "            'mean_green': float('nan'),\n",
    "            'mean_blue': float('nan'),\n",
    "            'std_red': float('nan'),\n",
    "            'std_green': float('nan'),\n",
    "            'std_blue': float('nan'),\n",
    "            'img_resolution': float('nan')\n",
    "        }\n",
    "        \n",
    "    return(out_dict)\n",
    "\n",
    "X_full = df\n",
    "x_full_with_images = parallelize_dataframe(X_full, append_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "feather.write_dataframe(x_full_with_images, 'img_df.feather')\n",
    "\n",
    "# Concatenate into a single DF\n",
    "train_df = pd.concat([X_full.reset_index(), x_full_with_images], axis=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [twosigma]",
   "language": "python",
   "name": "Python [twosigma]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
