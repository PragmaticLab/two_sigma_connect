{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Sigma Connect - Rental Listing Inqueries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle link](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Base dependencies\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import uuid\n",
    "\n",
    "# Machine learning / stats dependencies\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Image processing dependencies\n",
    "from PIL import Image\n",
    "from StringIO import StringIO\n",
    "\n",
    "# Neighborhood dependencies\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Suppress annoying deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the training data\n",
    "train_df = pd.read_json(\"train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "### What problem are we trying solve?\n",
    "* RentHop is an apartment search website. We are trying to predict the interest level (high, medium, low) of a new listing.\n",
    "* RentHop could use the model developed in this exercise to improve the quality of search results and therefore increase the frequency of bookings.\n",
    "* In addition, our analytic might help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand rentersâ€™ needs and preferences.\n",
    "\n",
    "### What are the relevant metrics? How much do we plan to improve them?\n",
    "* The evaluation metric is the multiclass loss, essentially logloss for 3 interest levels.\n",
    "* A baseline prediction of 0.33 for each class will result in a loss of 1.1. We plan on reducing the logloss to 0.7 or lower (or a prediction of 0.5 for the correct class, an almost 50% increase in confidence from the baseline prediction)\n",
    "\n",
    "### What will we deliver?\n",
    "* A categorical response prediction model for predicting the interest level of an apartment listing.\n",
    "* This prediction will primarily be used to rank apartments from the RentHop search page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "### What are the raw data sources?\n",
    "* The training data provided are raw listing details, provided in JSON format by RentHop.\n",
    "\n",
    "### What does each 'unit' (e.g. row) of data represent?\n",
    "* Each row is an apartment listing, containing internal apartment characteristics (like number of bathrooms) and contextual metadata (like lat-lon and street address)\n",
    "\n",
    "### What are the fields (columns)?\n",
    "* Dependent variable: \n",
    "    - *interest_level (categorical)*: 'High', 'Medium', or 'Low' rental interest, calculated by RentHop using an algorithm undisclosed to the public\n",
    "* Independent variable: \n",
    "    - *bathrooms (numeric)*: Number of bathrooms in the unit\n",
    "    - *bedrooms (numeric)*: Number of bedrooms in the unit\n",
    "    - *building_id (categorical)*: Unique ID for particular building\n",
    "    - *created (date_string)*: Date the listing was first created on RentHop\n",
    "    - *description (string)*: Open-text description of the unit, provided by the listing author\n",
    "    - *display_address (string)*: Marketing-friendly address (not strictly a Post Office address) like \"Studio at 5528-5532 S. Everett Avenue\"\n",
    "    - *features (string)*: Semi-structured list of features like \"gas stove\" and \"air conditioning\"\n",
    "    - *latitude (numeric)*: Latitude of the listed property\n",
    "    - *longitude (numeric)*: Longitude of the listed property\n",
    "    - *listing_id (categorical)*: Unique ID for a particular listing\n",
    "    - *manager_id (categorical)*: Unique ID for a building manager\n",
    "    - *photos (list of strings)*: List of URLs to listing photos on RentHop\n",
    "    - *price (numeric)*: Monthly rent price (in USD)\n",
    "    - *street_address (string)*: Actual street address of the listed property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'bathrooms', u'bedrooms', u'building_id', u'created', u'description',\n",
       "       u'display_address', u'features', u'interest_level', u'latitude',\n",
       "       u'listing_id', u'longitude', u'manager_id', u'photos', u'price',\n",
       "       u'street_address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show col names\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "* Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_df.isnull().any(axis=1)) # there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribution of target\n",
    "    * There are about 70% low interest, 23% medium interest and 8% high interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts of targets: \n",
      "low       34284\n",
      "medium    11229\n",
      "high       3839\n",
      "Name: interest_level, dtype: int64\n",
      "\n",
      "\n",
      "percentages for targets: \n",
      "low       69.468309\n",
      "medium    22.752877\n",
      "high       7.778813\n",
      "Name: interest_level, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print \"raw counts of targets: \"\n",
    "print train_df.interest_level.value_counts()\n",
    "\n",
    "print \"\\n\\npercentages for targets: \"\n",
    "print train_df.interest_level.value_counts() * 100.0 / train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### What steps are taken to prepare the data for modeling?\n",
    "* feature transformations? engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the target into integer encoding\n",
    "* Interest level (e.g. High, Medium, Low) was recoded to 0, 1, 2 for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_target_var(input_df):\n",
    "    \"\"\"\n",
    "    Generate simple numeric features derived from the raw data.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pandas dataframe): A dataframe of listing data.\n",
    "        \n",
    "    Returns:\n",
    "        A 1-D numpy array with values of the target variable (0, 1, 2) \n",
    "    \"\"\"\n",
    "    target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "    y = np.array(input_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Features from the Apartment Listing Description\n",
    "\n",
    "The listing data includes an open-text field called \"description\". Listing authors use this field to describe the listing in their own words. Because this field contains natural text in English, we attempted to extract some features from it using common tools from Natural Language Processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_tfidf_features(input_df, train_df, ngram_range = (1, 2), max_features = 1000):\n",
    "    \"\"\"\n",
    "    Extract some features from the open-text 'description' field\n",
    "    using a TF-IDF vectorizer.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pandas dataframe): A dataframe of listing data.\n",
    "        train_df (pandas dataframe): A dataframe of training data used \\\n",
    "            to fit the TF-IDF vectorizer.\n",
    "        ngram_range (tuple): An integer tuple of the form (lower_bound, upper_bound) \\\n",
    "            used to control the behavior of the vectorizer. For example, passing \\\n",
    "            (1, 2) will create 1-gram and 2-gram features.\n",
    "        max_features (int): The top <max_features> features from the vectorizer will be \\\n",
    "            preserved.\n",
    "            \n",
    "    Returns:\n",
    "        A pandas DF identical to input_df, but with the 1000 image features appended.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit a vectorizer\n",
    "    des = TfidfVectorizer(ngram_range=ngram_range, max_features = max_features, stop_words = 'english')\n",
    "    des.fit(train_df.description)\n",
    "    \n",
    "    # Use it to transform input_df\n",
    "    tfidf_features = des.transform(input_df.description)\n",
    "    \n",
    "    # Create a Pandas dataframe\n",
    "    colnames = ['tfidf_' + featname for featname in des.get_feature_names()]\n",
    "    tfidf_features = pd.DataFrame(tfidf_features.toarray(), columns = colnames)\n",
    "    \n",
    "    # Append and return\n",
    "    out_df = pd.concat([input_df.reset_index(), tfidf_features.reset_index()], axis = 1)\n",
    "    return(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing description Numerical features\n",
    "* Number of photos\n",
    "* Number of features (features are the tags provided by the listing, e.g. Doorman, Elevator, Fitness Center..etc)\n",
    "* Number of words in description\n",
    "* Year created\n",
    "* Month created\n",
    "* Day created\n",
    "* Hour creaed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_numeric_features(input_df):\n",
    "    \"\"\"\n",
    "    Generate simple numeric features derived from the raw data.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pandas dataframe): A dataframe of listing data.\n",
    "    \"\"\"\n",
    "    input_df[\"num_photos\"] = input_df[\"photos\"].apply(len)\n",
    "    input_df[\"num_features\"] = input_df[\"features\"].apply(len)\n",
    "    input_df[\"num_description_words\"] = input_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    input_df[\"created\"] = pd.to_datetime(input_df[\"created\"])\n",
    "    input_df[\"created_year\"] = input_df[\"created\"].dt.year\n",
    "    input_df[\"created_month\"] = input_df[\"created\"].dt.month\n",
    "    input_df[\"created_day\"] = input_df[\"created\"].dt.day\n",
    "    input_df[\"created_hour\"] = input_df[\"created\"].dt.hour\n",
    "    return(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Features Derived from Listing Images\n",
    "\n",
    "We use Python's PIL library to process the provided image files into numeric matrices. Each pixel in the provided image is encoded as a 3-element tuple representing RGB (red, green, blue) values. Each color's value is referred to as a \"channel\" in the image processing literature and in this report\n",
    "\n",
    "* Mean pixel value, red channel\n",
    "* Mean pixel value, green channel\n",
    "* Mean pixel value, blue channel\n",
    "* Standard deviation of pixel values, red channel\n",
    "* Standard deviation of pixel values, green channel\n",
    "* Standard deviation of pixel values, blue channel\n",
    "* Image resolution (total number of pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define helper functions for creating image features\n",
    "def parallelize_dataframe(df, func):\n",
    "    num_partitions = 250 #number of partitions to split dataframe\n",
    "    num_cores = 7 #number of cores on your machine\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def append_image_features(input_df):\n",
    "    img_features = input_df['photos'].map(lambda photo_album: get_image_features(photo_album))\n",
    "    img_df = pd.DataFrame({\n",
    "            'mean_red': np.array([feature_dict['mean_red'] for feature_dict in img_features]),\n",
    "            'mean_green': np.array([feature_dict['mean_green'] for feature_dict in img_features]),\n",
    "            'mean_blue': np.array([feature_dict['mean_blue'] for feature_dict in img_features]),\n",
    "            'std_red': np.array([feature_dict['std_red'] for feature_dict in img_features]),\n",
    "            'std_green': np.array([feature_dict['std_green'] for feature_dict in img_features]),\n",
    "            'std_blue': np.array([feature_dict['std_blue'] for feature_dict in img_features]),\n",
    "            'img_resolution': np.array([feature_dict['img_resolution'] for feature_dict in img_features])\n",
    "        })\n",
    "    return img_df\n",
    "\n",
    "def get_image_features(photo_url_list):\n",
    "    \"\"\"\n",
    "    Create one row of features for a collection of\n",
    "    images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write a temp file to disk to track progress\n",
    "    fname = '/Users/jlamb/repos/sandbox/tmp/' + str(uuid.uuid1())\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('x')\n",
    "    \n",
    "    if len(photo_url_list) > 0:\n",
    "        \n",
    "        try: \n",
    "            # Set up collectors\n",
    "            mean_red = []\n",
    "            mean_green = []\n",
    "            mean_blue = []\n",
    "            std_red = []\n",
    "            std_green = []\n",
    "            std_blue = []\n",
    "            img_resolution = []\n",
    "\n",
    "            # TESTING: Just use first image for now\n",
    "            photo_url_list = [photo_url_list[0]]\n",
    "            for url in photo_url_list:\n",
    "\n",
    "                # Get photo (http://stackoverflow.com/questions/7391945/how-do-i-read-image-data-from-a-url-in-python)\n",
    "                url = url\n",
    "                response = requests.get(url)\n",
    "                img = np.array(Image.open(StringIO(response.content)))\n",
    "\n",
    "                # Mean value by channel\n",
    "                mean_red.append(img[:,0].mean())\n",
    "                mean_green.append(img[:,1].mean())\n",
    "                mean_blue.append(img[:,2].mean())\n",
    "\n",
    "                # standard deviation by channel\n",
    "                std_red.append(img[:,0].std())\n",
    "                std_green.append(img[:,1].std())\n",
    "                std_blue.append(img[:,2].std())\n",
    "\n",
    "                # resolution (num pixels)\n",
    "                img_resolution.append(img.size)\n",
    "\n",
    "            # Summarize \n",
    "            out_dict = {\n",
    "                'mean_red': np.mean(np.array(mean_red)),\n",
    "                'mean_green': np.mean(np.array(mean_green)),\n",
    "                'mean_blue': np.mean(np.array(mean_blue)),\n",
    "                'std_red': np.mean(np.array(std_red)),\n",
    "                'std_green': np.mean(np.array(std_green)),\n",
    "                'std_blue': np.mean(np.array(std_blue)),\n",
    "                'img_resolution': np.mean(np.array(img_resolution))\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            out_dict = {\n",
    "                'mean_red': float('nan'),\n",
    "                'mean_green': float('nan'),\n",
    "                'mean_blue': float('nan'),\n",
    "                'std_red': float('nan'),\n",
    "                'std_green': float('nan'),\n",
    "                'std_blue': float('nan'),\n",
    "                'img_resolution': float('nan')\n",
    "            }\n",
    "            \n",
    "        \n",
    "    else:\n",
    "    \n",
    "        out_dict = {\n",
    "            'mean_red': float('nan'),\n",
    "            'mean_green': float('nan'),\n",
    "            'mean_blue': float('nan'),\n",
    "            'std_red': float('nan'),\n",
    "            'std_green': float('nan'),\n",
    "            'std_blue': float('nan'),\n",
    "            'img_resolution': float('nan')\n",
    "        }\n",
    "        \n",
    "    return(out_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags of the listing\n",
    "Extract the tags of each listing and output as sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    feature_list = []\n",
    "    for i in df.features:\n",
    "        feature_list.extend(i)\n",
    "    names = list(set(feature_list))\n",
    "    voc2id = dict(zip(names, range(len(names))))\n",
    "    rows, cols, vals = [], [], []\n",
    "    for r, d in enumerate(df.features):\n",
    "        for e in d:\n",
    "            if voc2id.get(e) is not None:\n",
    "                rows.append(r)\n",
    "                cols.append(voc2id[e])\n",
    "                vals.append(1)\n",
    "    features = sp.csr_matrix((vals, (rows, cols)))\n",
    "    svd = TruncatedSVD(n_components=8)\n",
    "    features_svd = pd.DataFrame(svd.fit_transform(features),columns = ['f-1','f-2','f-3','f-4','f-5','f-6','f-7','f-8'])\n",
    "    return features_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood features\n",
    "On Renthop website the lists are segments by neighborhoods. A list of neighborhood names from the Renthop nyc website. \n",
    "\n",
    "The latitude and longitude of the neighborhood is gotten with the geopy.geocodes. \n",
    "\n",
    "The corresponding neighborhood is returned by calculated the distance between the listing building and the center of the neighborhood.\n",
    "\n",
    "There are some wrong records in the records, which give me a location in Chicago or even farther. All listing with distance greater than 10 are therefore removed.\n",
    "\n",
    " There are a total of 158 neighborhoods in the Great New York Area on Renthop. They are converted to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighborhood(df):\n",
    "    m_neighborhoods = ['Battery Park City','Bowery','Chinatown','Civic Center','East Village','Financial District',\n",
    "                       'Greenwich Village','Little Italy','Lower East Side','NoHo','NoLita','SoHo','Tribeca','Two Bridges',\n",
    "                       'West Village','Chelsea','Flatiron District','Garment District','Gramercy Park',\"Hell's Kitchen\",'Kips Bay',\n",
    "                       'Koreatown','Midtown East','Murray Hill','NoMad','Stuyvesant Town - Peter Cooper Village','Theater District',\n",
    "                       'Central Harlem','Central Park','East Harlem','Inwood','Upper East Side','Upper West Side',\n",
    "                       'Washington Heights','West Harlem','Randalls-Wards Island','Roosevelt Island']\n",
    "\n",
    "    b_neighborhoods = ['Bedford-Stuyvesant','Bushwick','Greenpoint','Williamsburg','Boerum Hill','Carroll Gardens','Cobble Hill',\n",
    "                       'Gowanus','Greenwood Heights','Park Slope','Prospect Park','Red Hook','Sunset Park','Windsor Terrace',\n",
    "                       'Crown Heights','East Flatbush','Flatbush','Kensington','Midwood','Ocean Hill','Brooklyn Heights',\n",
    "                       'Brooklyn Navy Yard','Clinton Hill','DUMBO','Downtown Brooklyn','Fort Greene','Prospect Heights',\n",
    "                       'Vinegar Hill','Bath Beach','Bay Ridge','Bensonhurst','Borough Park','Dyker Heights','Mapleton',\n",
    "                       'Brighton Beach','Coney Island','Gravesend','Sheepshead Bay','Brownsville','Canarsie','Cypress Hills',\n",
    "                       'East New York','Bergen Beach','Flatlands','Floyd Bennett Airfield','Marine Park','Mill Basin']\n",
    "\n",
    "    q_neighborhoods = ['Astoria','Corona','East Elmhurst','Elmhurst','Forest Hills','Glendale','Jackson Heights','Long Island City',\n",
    "                       'Maspeth','Middle Village','Rego Park','Ridgewood','Sunnyside','Woodside','Auburndale','Bayside',\n",
    "                       'College Point','Flushing','Flushing Meadows-Corona Park','Fresh Meadows','Glen Oaks','Kew Gardens',\n",
    "                       'Kew Gardens Hills','Whitestone','Briarwood','Hollis','Holliswood','Jamaica','Jamaica Estates',\n",
    "                       'Jamaica Hills','South Jamaica','St. Albans','Forest Park','Howard Beach','Ozone Park','Richmond Hill',\n",
    "                       'South Ozone Park','Woodhaven','Far Rockaway','Rockaway Beach']\n",
    "\n",
    "    s_neighborhoods =  ['East Shore','Mid-Island','North Shore','South Shore']\n",
    "\n",
    "    bx_neighborhoods = ['Bedford Park','Belmont','Bronx Park','Concourse','Concourse Village','East Tremont','Fordham Heights',\n",
    "                        'Fordham Manor','Highbridge','Hunts Point','Kingsbridge','Longwood','Marble Hill','Morris Heights',\n",
    "                        'Morrisania','Mott Haven','Mount Eden','Mount Hope','Norwood','Riverdale','University Heights',\n",
    "                        'Van Cortlandt Park','West Farms','Allerton','Clason Point','Morris Park','Parkchester','Pelham Bay',\n",
    "                        'Pelham Parkway','Throgs Neck','Unionport','Van Nest','Wakefield','Westchester Village','Williamsbridge',\n",
    "                        'Woodlawn Heights']\n",
    "\n",
    "    nj_neighborhoods = ['Bergen - Lafayette',\n",
    "                        'Greenville','Historic Downtown','McGinley Square','The Heights','The Waterfront','West Side']\n",
    "    neighborhood_list = ['m_neighborhoods','b_neighborhoods','q_neighborhoods','s_neighborhoods','bx_neighborhoods','nj_neighborhoods']\n",
    "    n_dict = {'m_neighborhoods':'manhattan','b_neighborhoods':'brooklyn','q_neighborhoods':'queens',\n",
    "          's_neighborhoods':'staten island','bx_neighborhoods':'bronx','nj_neighborhoods':'new jersey'}\n",
    "    neighborhood_dict= {}\n",
    "    for n_list in neighborhood_list:\n",
    "        for n in eval(n_list):\n",
    "            geolocator = Nominatim()\n",
    "            location = geolocator.geocode(n+\" \"+n_dict[n_list], timeout=3)\n",
    "            try:\n",
    "                neighborhood_dict[n]=(location.latitude, location.longitude)\n",
    "            except:\n",
    "                continue\n",
    "    neighborhood_dict['Nolita'] = (40.7230413,-73.9948607)\n",
    "    neighborhood_dict['Two Bridges'] = (40.71074689999999,-73.99696)\n",
    "    neighborhood_dict['Flatiron District'] = (40.74008300000001,-73.99034890000001)\n",
    "    neighborhood_dict['Garment District'] = (40.7547072,-73.99163420000002)\n",
    "    neighborhood_dict['Stuyvesant Town - Peter Cooper Village'] = (40.7316903,-73.97784939999997)\n",
    "    neighborhood_dict['Randalls-Wards Island'] = (40.7932271,-73.92128579999996)\n",
    "    neighborhood_dict['Floyd Bennett Airfield'] = (40.5910174,-73.8906091)\n",
    "    neighborhood_dict['Jamaica Hills'] = (40.71280290000001,-73.79926339999997)\n",
    "    neighborhood_dict['East Shore'] = (40.606221,-74.06419489999996)\n",
    "    neighborhood_dict['Mid-Island'] = (40.5993847,-74.17931770000001)\n",
    "    neighborhood_dict['North Shore'] =(40.62604414772478,-74.12667847936973)\n",
    "    neighborhood_dict['South Shore'] = (40.531910867376595,-74.21559907263145)\n",
    "    neighborhood_dict['Westchester Village'] = (40.8407103,-73.8473591)\n",
    "    neighborhood_dict['Bergen - Lafayette'] =(40.71114639999999,-74.07407289999998)\n",
    "    neighborhood_dict['Historic Downtown'] =(40.71114639999999,-74.07407289999998)\n",
    "    neighborhood_dict['McGinley Square'] = (40.7241223,-74.06966729999999)\n",
    "    neighborhood_dict['West Side'] =(40.7247507,-74.08288390000001)\n",
    "    neighborhood_df = pd.Series(neighborhood_dict)\n",
    "    dist = DistanceMetric.get_metric('euclidean')\n",
    "    X = list(neighborhood_df)\n",
    "\n",
    "    min_distance = np.zeros(df.shape[0])\n",
    "    min_index = []\n",
    "    count = 0\n",
    "    for listing in np.array(df[['latitude','longitude']]) :   \n",
    "        min_distance[count] = np.min(dist.pairwise(X,[listing]), axis = 0)\n",
    "        if np.min(dist.pairwise(X,[listing]), axis = 0) < 10:\n",
    "            min_index.append(neighborhood_df.index[np.argmin(dist.pairwise(X,[listing]))])\n",
    "        else:\n",
    "            min_index.append(np.nan)\n",
    "        count += 1\n",
    "    min_index = np.array(min_index)\n",
    "    n_df = pd.get_dummies(min_index)\n",
    "    n_df = n_df.fillna(0)\n",
    "    return n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Modeling Base Tables\n",
    "\n",
    "With these feature functions in hand, we can create the tables used in the modeling effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create train, validation, full_train, and testing\n",
    "# --- *_full_train = the entire training dataset (used to fit final model)\n",
    "# --- *_dev_train  = training set used during model development\n",
    "# --- *_dev_val    = validation data used during development\n",
    "# --- *_test       = test set that we will call predict on to produce final submission\n",
    "\n",
    "# Read in raw data\n",
    "full_train = pd.read_json(\"train.json\")\n",
    "X_test  = pd.read_json(\"test.json\")\n",
    "\n",
    "# Add features that can be added before train-test split\n",
    "\n",
    "# --- 1. Image features\n",
    "# [training data] Apply the image processing or readin cached features\n",
    "if not os.path.isfile('img_full_train.pickle'):\n",
    "    img_full_train = parallelize_dataframe(full_train, append_image_features)\n",
    "    pickle.dump(img_full_train, open('img_full_train.pickle', 'wb'))\n",
    "else:\n",
    "    with open('img_full_train.pickle', 'r') as pickle_file:\n",
    "        img_full_train = pickle.load(pickle_file)\n",
    "full_train = pd.concat([full_train.reset_index(), img_full_train.reset_index()], axis = 1)\n",
    "\n",
    "# [testing data] Apply the image processing or readin cached features\n",
    "if not os.path.isfile('img_test.pickle'):\n",
    "    img_test = parallelize_dataframe(X_test, append_image_features)\n",
    "    pickle.dump(img_test, open('img_test.pickle', 'wb'))\n",
    "else:\n",
    "    with open('img_test.pickle', 'r') as pickle_file:\n",
    "        img_test = pickle.load(pickle_file)\n",
    "X_test = pd.concat([X_test.reset_index(), img_test.reset_index()], axis = 1)\n",
    "\n",
    "# --- 2. Numeric features\n",
    "full_train = add_numeric_features(full_train)\n",
    "X_test = add_numeric_features(X_test)\n",
    "\n",
    "# --- 3 Add listing tags\n",
    "features_svd_train = feature_extraction(full_train)\n",
    "features_svd_test = feature_extraction(X_test)\n",
    "\n",
    "# --- 4 Add neighborhood features\n",
    "\n",
    "# Sometimes this needs to be retried a few times,\n",
    "# server is real unreliable\n",
    "try:\n",
    "    n_df_train = neighborhood(full_train)\n",
    "except:\n",
    "    n_df_train = neighborhood(full_train)\n",
    "\n",
    "try:\n",
    "    n_df_test = neighborhood(X_test)\n",
    "except:\n",
    "    n_df_test = neighborhood(X_test)\n",
    "\n",
    "frames = [full_train, features_svd_train,n_df_train]\n",
    "full_train = pd.concat(frames, axis=1)\n",
    "\n",
    "frames_test = [X_test, features_svd_test,n_df_test]\n",
    "X_test = pd.concat(frames_test, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "####===== Split into X & Y =====####\n",
    "Y_full_train = get_target_var(full_train)\n",
    "\n",
    "# Split train_df into train and validation\n",
    "X_dev_train, X_dev_val, Y_dev_train, Y_dev_val = train_test_split(full_train, Y_full_train, test_size=0.33)\n",
    "\n",
    "# Add NLP features\n",
    "X_dev_train = add_tfidf_features(X_dev_train, X_dev_train, ngram_range = (1,2), max_features = 1000)\n",
    "X_dev_val = add_tfidf_features(X_dev_val, X_dev_train, ngram_range = (1,2), max_features = 1000)\n",
    "X_test = add_tfidf_features(X_test, full_train, ngram_range = (1,2), max_features = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precise description of modeling base tables.\n",
    "* What are the rows/columns of X (the predictors)?\n",
    "* Target variable:\n",
    "    - *interest_level (categorical)*: A three-level categorical variable. Encoded as 0, 1, 2 corresponding with levels \"low\", \"medium\" and \"high\" interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "### What model are we using? Why?\n",
    "### Assumptions?\n",
    "### Regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "### How well does the model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Implementing a deployed analytic is outside the scope of this exercise. The answers below are hypothetical only.\n",
    "\n",
    "### How is the model deployed?\n",
    "* To be deployed at RentHop, it's likely that the model would need to be deployed as a standalone microservice which can be managed by Operational personnell. Regardless of the exact technology used, the application should expect to receive a JSON payload with raw listing details and should produce a small JSON objects with probabilities for each class ('low', 'medium', 'high'). We see a few possible configurations that could support such an app:\n",
    "    1. All-Python app (e.g. Flask) listening for POST requests with listing data\n",
    "    2. Containerized Python app (e.g. in Docker) in a container which also runs a web server\n",
    "    3. Python model rewritten in Java by engineers\n",
    "\n",
    "### What support is provided after initial deployment?\n",
    "* Model results will be tracked by nightly batch jobs to try to catch eroding environmental fit\n",
    "* The model may need to be updated periodically to refelect a changing renatl environment or to incorporate new data sources"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (twosigma)",
   "language": "python",
   "name": "twosigma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
