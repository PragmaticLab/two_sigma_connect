{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Sigma Connect - Rental Listing Inqueries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle link](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/anaconda3/envs/twosigma/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Base dependencies\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import uuid\n",
    "\n",
    "# Machine learning / stats dependencies\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Image processing dependencies\n",
    "from PIL import Image\n",
    "from StringIO import StringIO\n",
    "\n",
    "# Suppress annoying deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the training data\n",
    "train_df = pd.read_json(\"train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "### What problem are we trying solve?\n",
    "* RentHop is an apartment search website. We are trying to predict the interest level (high, medium, low) of a new listing.\n",
    "* RentHop could use the model developed in this exercise to improve the quality of search results and therefore increase the frequency of bookings.\n",
    "* In addition, our analytic might help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand rentersâ€™ needs and preferences.\n",
    "\n",
    "### What are the relevant metrics? How much do we plan to improve them?\n",
    "* The evaluation metric is the multiclass loss, essentially logloss for 3 interest levels.\n",
    "* A baseline prediction of 0.33 for each class will result in a loss of 1.1. We plan on reducing the logloss to 0.7 or lower (or a prediction of 0.5 for the correct class, an almost 50% increase in confidence from the baseline prediction)\n",
    "\n",
    "### What will we deliver?\n",
    "* A categorical response prediction model for predicting the interest level of an apartment listing.\n",
    "* This prediction will primarily be used to rank apartments from the RentHop search page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "### What are the raw data sources?\n",
    "* The training data provided are raw listing details, provided in JSON format by RentHop.\n",
    "\n",
    "### What does each 'unit' (e.g. row) of data represent?\n",
    "* Each row is an apartment listing, containing internal apartment characteristics (like number of bathrooms) and contextual metadata (like lat-lon and street address)\n",
    "\n",
    "### What are the fields (columns)?\n",
    "* Dependent variable: \n",
    "    - *interest_level (categorical)*: 'High', 'Medium', or 'Low' rental interest, calculated by RentHop using an algorithm undisclosed to the public\n",
    "* Independent variable: \n",
    "    - *bathrooms (numeric)*: Number of bathrooms in the unit\n",
    "    - *bedrooms (numeric)*: Number of bedrooms in the unit\n",
    "    - *building_id (categorical)*: Unique ID for particular building\n",
    "    - *created (date_string)*: Date the listing was first created on RentHop\n",
    "    - *description (string)*: Open-text description of the unit, provided by the listing author\n",
    "    - *display_address (string)*: Marketing-friendly address (not strictly a Post Office address) like \"Studio at 5528-5532 S. Everett Avenue\"\n",
    "    - *features (string)*: Semi-structured list of features like \"gas stove\" and \"air conditioning\"\n",
    "    - *latitude (numeric)*: Latitude of the listed property\n",
    "    - *longitude (numeric)*: Longitude of the listed property\n",
    "    - *listing_id (categorical)*: Unique ID for a particular listing\n",
    "    - *manager_id (categorical)*: Unique ID for a building manager\n",
    "    - *photos (list of strings)*: List of URLs to listing photos on RentHop\n",
    "    - *price (numeric)*: Monthly rent price (in USD)\n",
    "    - *street_address (string)*: Actual street address of the listed property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'bathrooms', u'bedrooms', u'building_id', u'created', u'description',\n",
       "       u'display_address', u'features', u'interest_level', u'latitude',\n",
       "       u'listing_id', u'longitude', u'manager_id', u'photos', u'price',\n",
       "       u'street_address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show col names\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "* Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_df.isnull().any(axis=1)) # there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribution of target\n",
    "    * There are about 70% low interest, 23% medium interest and 8% high interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts of targets: \n",
      "low       34284\n",
      "medium    11229\n",
      "high       3839\n",
      "Name: interest_level, dtype: int64\n",
      "\n",
      "\n",
      "percentages for targets: \n",
      "low       69.468309\n",
      "medium    22.752877\n",
      "high       7.778813\n",
      "Name: interest_level, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print \"raw counts of targets: \"\n",
    "print train_df.interest_level.value_counts()\n",
    "\n",
    "print \"\\n\\npercentages for targets: \"\n",
    "print train_df.interest_level.value_counts() * 100.0 / train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### What steps are taken to prepare the data for modeling?\n",
    "* feature transformations? engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the target into integer encoding\n",
    "* Interest level (e.g. High, Medium, Low) was recoded to 0, 1, 2 for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_target_var(input_df):\n",
    "    \"\"\"\n",
    "    Generate simple numeric features derived from the raw data.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pandas dataframe): A dataframe of listing data.\n",
    "        \n",
    "    Returns:\n",
    "        A 1-D numpy array with values of the target variable (0, 1, 2) \n",
    "    \"\"\"\n",
    "    target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "    y = np.array(input_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Features from the Apartment Listing Description\n",
    "\n",
    "The listing data includes an open-text field called \"description\". Listing authors use this field to describe the listing in their own words. Because this field contains natural text in English, we attempted to extract some features from it using common tools from Natural Language Processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_tfidf_features(input_df, train_df, ngram_range = (1, 2), max_features = 1000):\n",
    "    \"\"\"\n",
    "    Extract some features from the open-text 'description' field\n",
    "    using a TF-IDF vectorizer.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pandas dataframe): A dataframe of listing data.\n",
    "        train_df (pandas dataframe): A dataframe of training data used \\\n",
    "            to fit the TF-IDF vectorizer.\n",
    "        ngram_range (tuple): An integer tuple of the form (lower_bound, upper_bound) \\\n",
    "            used to control the behavior of the vectorizer. For example, passing \\\n",
    "            (1, 2) will create 1-gram and 2-gram features.\n",
    "        max_features (int): The top <max_features> features from the vectorizer will be \\\n",
    "            preserved.\n",
    "            \n",
    "    Returns:\n",
    "        A pandas DF identical to input_df, but with the 1000 image features appended.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit a vectorizer\n",
    "    des = TfidfVectorizer(ngram_range=ngram_range, max_features = max_features, stop_words = 'english')\n",
    "    des.fit(train_df.description)\n",
    "    \n",
    "    # Use it to transform input_df\n",
    "    tfidf_features = des.transform(input_df.description)\n",
    "    \n",
    "    # Create a Pandas dataframe\n",
    "    colnames = ['tfidf_' + featname for featname in des.get_feature_names()]\n",
    "    tfidf_features = pd.DataFrame(tfidf_features.toarray(), columns = colnames)\n",
    "    \n",
    "    # Append and return\n",
    "    out_df = pd.concat([input_df.reset_index(), tfidf_features.reset_index()], axis = 1)\n",
    "    return(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing description Numerical features\n",
    "* Number of photos\n",
    "* Number of features (features are the tags provided by the listing, e.g. Doorman, Elevator, Fitness Center..etc)\n",
    "* Number of words in description\n",
    "* Year created\n",
    "* Month created\n",
    "* Day created\n",
    "* Hour creaed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_numeric_features(input_df)\n",
    "    \"\"\"\n",
    "    Generate simple numeric features derived from the raw data.\n",
    "    \n",
    "    Args:\n",
    "        input_df (pandas dataframe): A dataframe of listing data.\n",
    "    \"\"\"\n",
    "    input_df[\"num_photos\"] = input_df[\"photos\"].apply(len)\n",
    "    input_df[\"num_features\"] = input_df[\"features\"].apply(len)\n",
    "    input_df[\"num_description_words\"] = input_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    input_df[\"created\"] = pd.to_datetime(input_df[\"created\"])\n",
    "    input_df[\"created_year\"] = input_df[\"created\"].dt.year\n",
    "    input_df[\"created_month\"] = input_df[\"created\"].dt.month\n",
    "    input_df[\"created_day\"] = input_df[\"created\"].dt.day\n",
    "    input_df[\"created_hour\"] = input_df[\"created\"].dt.hour\n",
    "    return(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Features Derived from Listing Images\n",
    "\n",
    "We use Python's PIL library to process the provided image files into numeric matrices. Each pixel in the provided image is encoded as a 3-element tuple representing RGB (red, green, blue) values. Each color's value is referred to as a \"channel\" in the image processing literature and in this report\n",
    "\n",
    "* Mean pixel value, red channel\n",
    "* Mean pixel value, green channel\n",
    "* Mean pixel value, blue channel\n",
    "* Standard deviation of pixel values, red channel\n",
    "* Standard deviation of pixel values, green channel\n",
    "* Standard deviation of pixel values, blue channel\n",
    "* Image resolution (total number of pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions for creating image features\n",
    "def parallelize_dataframe(df, func):\n",
    "    num_partitions = 250 #number of partitions to split dataframe\n",
    "    num_cores = 7 #number of cores on your machine\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def append_image_features(input_df):\n",
    "    img_features = input_df['photos'].map(lambda photo_album: get_image_features(photo_album))\n",
    "    img_df = pd.DataFrame({\n",
    "            'mean_red': np.array([feature_dict['mean_red'] for feature_dict in img_features]),\n",
    "            'mean_green': np.array([feature_dict['mean_green'] for feature_dict in img_features]),\n",
    "            'mean_blue': np.array([feature_dict['mean_blue'] for feature_dict in img_features]),\n",
    "            'std_red': np.array([feature_dict['std_red'] for feature_dict in img_features]),\n",
    "            'std_green': np.array([feature_dict['std_green'] for feature_dict in img_features]),\n",
    "            'std_blue': np.array([feature_dict['std_blue'] for feature_dict in img_features]),\n",
    "            'img_resolution': np.array([feature_dict['img_resolution'] for feature_dict in img_features])\n",
    "        })\n",
    "    return img_df\n",
    "\n",
    "def get_image_features(photo_url_list):\n",
    "    \"\"\"\n",
    "    Create one row of features for a collection of\n",
    "    images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write a temp file to disk to track progress\n",
    "    fname = '/Users/jlamb/repos/sandbox/tmp/' + str(uuid.uuid1())\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('x')\n",
    "    \n",
    "    if len(photo_url_list) > 0:\n",
    "        \n",
    "        try: \n",
    "            # Set up collectors\n",
    "            mean_red = []\n",
    "            mean_green = []\n",
    "            mean_blue = []\n",
    "            std_red = []\n",
    "            std_green = []\n",
    "            std_blue = []\n",
    "            img_resolution = []\n",
    "\n",
    "            # TESTING: Just use first image for now\n",
    "            photo_url_list = [photo_url_list[0]]\n",
    "            for url in photo_url_list:\n",
    "\n",
    "                # Get photo (http://stackoverflow.com/questions/7391945/how-do-i-read-image-data-from-a-url-in-python)\n",
    "                url = url\n",
    "                response = requests.get(url)\n",
    "                img = np.array(Image.open(StringIO(response.content)))\n",
    "\n",
    "                # Mean value by channel\n",
    "                mean_red.append(img[:,0].mean())\n",
    "                mean_green.append(img[:,1].mean())\n",
    "                mean_blue.append(img[:,2].mean())\n",
    "\n",
    "                # standard deviation by channel\n",
    "                std_red.append(img[:,0].std())\n",
    "                std_green.append(img[:,1].std())\n",
    "                std_blue.append(img[:,2].std())\n",
    "\n",
    "                # resolution (num pixels)\n",
    "                img_resolution.append(img.size)\n",
    "\n",
    "            # Summarize \n",
    "            out_dict = {\n",
    "                'mean_red': np.mean(np.array(mean_red)),\n",
    "                'mean_green': np.mean(np.array(mean_green)),\n",
    "                'mean_blue': np.mean(np.array(mean_blue)),\n",
    "                'std_red': np.mean(np.array(std_red)),\n",
    "                'std_green': np.mean(np.array(std_green)),\n",
    "                'std_blue': np.mean(np.array(std_blue)),\n",
    "                'img_resolution': np.mean(np.array(img_resolution))\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            out_dict = {\n",
    "                'mean_red': float('nan'),\n",
    "                'mean_green': float('nan'),\n",
    "                'mean_blue': float('nan'),\n",
    "                'std_red': float('nan'),\n",
    "                'std_green': float('nan'),\n",
    "                'std_blue': float('nan'),\n",
    "                'img_resolution': float('nan')\n",
    "            }\n",
    "            \n",
    "        \n",
    "    else:\n",
    "    \n",
    "        out_dict = {\n",
    "            'mean_red': float('nan'),\n",
    "            'mean_green': float('nan'),\n",
    "            'mean_blue': float('nan'),\n",
    "            'std_red': float('nan'),\n",
    "            'std_green': float('nan'),\n",
    "            'std_blue': float('nan'),\n",
    "            'img_resolution': float('nan')\n",
    "        }\n",
    "        \n",
    "    return(out_dict)\n",
    "\n",
    "# Append image features to train_df (equivalent to R 'cbind')\n",
    "train_df = pd.concat([train_df.reset_index(), img_df.reset_index()], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Modeling Base Tables\n",
    "\n",
    "With these feature functions in hand, we can create the tables used in the modeling effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train, validation, full_train, and testing\n",
    "# --- *_full_train = the entire training dataset (used to fit final model)\n",
    "# --- *_dev_train  = training set used during model development\n",
    "# --- *_dev_val    = validation data used during development\n",
    "# --- *_test       = test set that we will call predict on to produce final submission\n",
    "\n",
    "# Read in raw data\n",
    "full_train = pd.read_json(\"train.json\")\n",
    "X_test  = pd.read_json(\"test_df\")\n",
    "\n",
    "# Add features that can be added before train-test split\n",
    "\n",
    "# --- 1. Image features\n",
    "# [training data] Apply the image processing or readin cached features\n",
    "if not os.path.isfile('img_full_train.pickle'):\n",
    "    img_full_train = parallelize_dataframe(full_train, append_image_features)\n",
    "    pickle.dump(img_full_train, open('img_full_train.pickle', 'wb'))\n",
    "else:\n",
    "    with open('img_full_train.pickle', 'r') as pickle_file:\n",
    "        img_full_train = pickle.load(pickle_file)\n",
    "full_train = pd.concat([full_train.reset_index(), img_full_train.reset_index()], axis = 1)\n",
    "\n",
    "# [testing data] Apply the image processing or readin cached features\n",
    "if not os.path.isfile('img_test.pickle'):\n",
    "    img_test = parallelize_dataframe(X_test, append_image_features)\n",
    "    pickle.dump(img_test, open('img_test.pickle', 'wb'))\n",
    "else:\n",
    "    with open('img_test.pickle', 'r') as pickle_file:\n",
    "        img_test = pickle.load(pickle_file)\n",
    "X_test = pd.concat([X_test.reset_index(), img_test.reset_index()], axis = 1)\n",
    "\n",
    "# --- 2. Numeric features\n",
    "full_train = add_numeric_features(full_train)\n",
    "X_test = add_numeric_features(X_test)\n",
    "\n",
    "# Split into X & Y\n",
    "Y_full_train = get_target_var(full_train)\n",
    "\n",
    "# Split train_df into train and validation\n",
    "X_dev_train, X_dev_val, Y_dev_train, Y_dev_val = train_test_split(full_train, Y_full_train, test_size=0.33)\n",
    "\n",
    "# Add NLP features\n",
    "X_dev_train = add_tfidf_features(X_dev_train, X_dev_train, ngram_range = (1,2), max_features = 1000)\n",
    "X_dev_val = add_tfidf_features(X_dev_val, X_dev_train, ngram_range = (1,2), max_features = 1000)\n",
    "X_test = add_tfidf_features(X_test, full_train, ngram_range = (1,2), max_features = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precise description of modeling base tables.\n",
    "* What are the rows/columns of X (the predictors)?\n",
    "* Target variable:\n",
    "    - *interest_level (categorical)*: A three-level categorical variable. Encoded as 0, 1, 2 corresponding with levels \"low\", \"medium\" and \"high\" interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "### What model are we using? Why?\n",
    "### Assumptions?\n",
    "### Regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "### How well does the model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Implementing a deployed analytic is outside the scope of this exercise. The answers below are hypothetical only.\n",
    "\n",
    "### How is the model deployed?\n",
    "* To be deployed at RentHop, it's likely that the model would need to be deployed as a standalone microservice which can be managed by Operational personnell. Regardless of the exact technology used, the application should expect to receive a JSON payload with raw listing details and should produce a small JSON objects with probabilities for each class ('low', 'medium', 'high'). We see a few possible configurations that could support such an app:\n",
    "    1. All-Python app (e.g. Flask) listening for POST requests with listing data\n",
    "    2. Containerized Python app (e.g. in Docker) in a container which also runs a web server\n",
    "    3. Python model rewritten in Java by engineers\n",
    "\n",
    "### What support is provided after initial deployment?\n",
    "* Model results will be tracked by nightly batch jobs to try to catch eroding environmental fit\n",
    "* The model may need to be updated periodically to refelect a changing renatl environment or to incorporate new data sources"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (twosigma)",
   "language": "python",
   "name": "twosigma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
